### 4.2 实验结果

#### 4.2.1 第一阶段

​		第一组实验考察数据增强方面的相关算法，选取了最具代表性的基础数据增强方法（记为**simplea**）、**mixup**方法、**labelsmoothing**方法作为实验组，另外设置一组不采用任何增强方法作为对照组（记为**noa**）。基础数据增强选用了随机旋转(40°以内)、水平垂直移动(比例在0.2以内)、水平翻转；mixup中λ设定为默认值0.2；labelsmoothing中![img](file:///C:/Users/QINYUH~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png)设定为默认0.1；实验结果如图5(1)、图5(2)：

| ![img](https://cdn.jsdelivr.net/gh/QYHcrossover/blog-imgbed/blogimg/20200707121645.png)  图 5(1) 数据增强方法loss-epoch图 | ![img](https://cdn.jsdelivr.net/gh/QYHcrossover/blog-imgbed/blogimg/20200707121655.png)  图 5(1) 数据增强方法valloss-epoch图 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
|                                                              |                                                              |

​		从实验结果上看基础增强方法和mixup方法都取得了不错的效果，基础增强方法在验证集上损失值最小，可谓泛化性能最佳，这也和本次实验使用了多种基础增强的手段有关；mixup略有逊色，不过泛化效果提升依旧明显，并且相比基础数据增强在训练集拟合效果更佳；而未做任何增强操作的对照组发生了非常严重的过拟合现象；labelsmoothing在本次实验中对泛化性能的提升并不明显，这可能和训练集本身精度高、样本类别过少有关系。

​		第二组实验考查模型约束相关方面的算法，选取了**l2正则化**、**dropout方法**；分别对l2正则化的超参数λ取0.1和0.25(分别记为l2-01和l2-025)，对dropout 中超参数p取0.1和0.25(分别记为l2-01和l2-025)，同时设置对照组作为实验参照。实验结果如图6(1)、图6(2)所示:

| ![img](https://cdn.jsdelivr.net/gh/QYHcrossover/blog-imgbed/blogimg/20200707121704.png)  图6 (1) 模型约束方法loss-epoch图 | ![img](https://cdn.jsdelivr.net/gh/QYHcrossover/blog-imgbed/blogimg/20200707121711.png)     图6 (2) 模型约束方法valloss-epoch图 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
|                                                              |                                                              |

​		从实验结果上看，无论是l2正则化还是dropout都有效地改善了正则化现象，l2正则化超参数的取值对验证集结果影响不大。dropout-0.25表现最佳，且领先优势十分明显；dropout-0.1与dropout-0.25差异明显。最后无论何种算法，适当增加模型约束的程度都会导致训练误差的增大，而验证误差则会相应减小。

#### 4.2.2 第二阶段

​		此阶段选择第一阶段表现优异的算法进行综合比较，以探究解决过拟合的两大方向是否存在明显的优劣之分；此阶段只进行valloss的对比，结果如图7所示:

![img](https://cdn.jsdelivr.net/gh/QYHcrossover/blog-imgbed/blogimg/20200707121717.png)

<center>图7 纵向对比valloss-epoch图</center>

​		实验结果表明，数据增强和模型约束对解决过拟合问题具有相似的效果，可谓殊途同归，并不存在明显的优劣之分。也可看出，在cifar-10数据集上，dropout和基础数组增强相较于其他方法而言具有最好的泛化效果；mixup方法和l2正则化次之，不过相比于未经过任何处理的对照组提升依旧明显。

​		最后我们尝试将两个方向的最优算法融合，既采用基础数据增强、又采用dropout方法随机失活，以探究其泛化性能是否更加出色，实验结果如图8：

![img](https://cdn.jsdelivr.net/gh/QYHcrossover/blog-imgbed/blogimg/20200707121724.png)

<center>图 8 融合情况对比图</center>

​		实验结果表明，简单将两个方向的方法融合在此次实验中并不能产生更优的表现。

### 4.3 实验总结

​		此次实验结果基本验证了理论假设的正确性，上述方法均在不同程度上改善了过拟合问题；数据增强和模型约束均是应对过拟合现象的有效手段，且这两个方向并不存在明显优劣。

​		训练误差和验证误差往往不可兼得，防止过拟合需要保证训练不能过分低；dropout和基础数据增强在此次实验中表现优异，但是并不能说明其一定优于其他方式。

 