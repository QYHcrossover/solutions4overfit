{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入相应的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\annocoda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "np.random.seed(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "(x_img_train,y_label_train),(x_img_test,y_label_test)=cifar10.load_data()\n",
    "x_img_train_normalize = x_img_train.astype('float32') / 255.0\n",
    "x_img_test_normalize = x_img_test.astype('float32') / 255.0\n",
    " \n",
    "from keras.utils import np_utils\n",
    "y_label_train_OneHot = np_utils.to_categorical(y_label_train)\n",
    "y_label_test_OneHot = np_utils.to_categorical(y_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 构造优化器\n",
    "from keras.optimizers import Adam,SGD,Adamax\n",
    "from keras.callbacks import CSVLogger,TensorBoard\n",
    "sgd = SGD(lr=0.01)\n",
    "adam = Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method1 从数据出发解决过拟合问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixup迭代实现\n",
    "class MixupGenerator():\n",
    "    def __init__(self, X_train, y_train, batch_size=32, alpha=0.2, shuffle=True, datagen=None):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(X_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        _, h, w, c = self.X_train.shape\n",
    "        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "        y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
    "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
    "        X = X1 * X_l + X2 * (1 - X_l)\n",
    "\n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建图像处理的迭代器\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    " \n",
    "#图片生成器\n",
    "noa_gen = ImageDataGenerator()\n",
    "simple_gen = ImageDataGenerator(rotation_range=40,\n",
    "                              width_shift_range=0.2,\n",
    "                              height_shift_range=0.2,\n",
    "                              horizontal_flip=True,\n",
    "                              fill_mode='nearest')\n",
    "noa_it = noa_gen.flow(x=x_img_train_normalize,y=y_label_train_OneHot,batch_size=32)\n",
    "simple_it = simple_gen.flow(x=x_img_train_normalize,y=y_label_train_OneHot,batch_size=32)\n",
    "mixup_it = MixupGenerator(x_img_train_normalize,y_label_train_OneHot)()\n",
    "iterator_dict = {\"noa\":noa_it,\n",
    "                \"simplea\":simple_it,\n",
    "                \"mixup\":mixup_it}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 1.4509 - acc: 0.4829 - val_loss: 1.1820 - val_acc: 0.5829\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 1.0920 - acc: 0.6156 - val_loss: 1.0329 - val_acc: 0.6410cc: - ETA: 7s - loss: 1.1108 - acc: 0.60 - ETA: 7s - loss: 1.1100 - acc - ETA: 7s - loss: 1.1096 - acc - ETA: 6s - loss: 1.1075 -  - ETA: 5s - loss: 1.1061 - ETA: 5s - loss: 1.1034 - ac - ETA: 4s - loss: 1.1025 - acc: 0.6 - ETA: 4s - loss: 1.1014  - ETA: 3s - loss: 1 - ETA: 0s - loss: 1.0919 - acc: 0\n",
      "Epoch 3/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.9446 - acc: 0.6685 - val_loss: 0.9370 - val_acc: 0.6774 - ETA: 8s - loss: 0.9583 - ETA: 7s - loss: 0.9554 - acc:  - ETA: 7s - loss: 0. -  - ETA: 4s -  - ETA: 3s - loss: 0.9467 - acc: 0.6 - ETA: 2s - loss: 0.9467 -\n",
      "Epoch 4/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.8488 - acc: 0.7026 - val_loss: 0.9069 - val_acc: 0.6892ss: 0.848 - ETA:  - ETA: 5s - lo\n",
      "Epoch 5/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.7785 - acc: 0.7280 - val_loss: 0.8932 - val_acc: 0.6895 0s - loss: 0.7781 \n",
      "Epoch 6/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.7117 - acc: 0.7516 - val_loss: 0.8899 - val_acc: 0.6915\n",
      "Epoch 7/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.6507 - acc: 0.7712 - val_loss: 0.8630 - val_acc: 0.7082 ETA: 11s - lo - ETA: 8s -  - ETA: 7s - loss: 0.6476 - ETA: 6s - loss: 0 - ETA: 5s - loss: 0.647\n",
      "Epoch 8/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.5939 - acc: 0.7953 - val_loss: 0.8819 - val_acc: 0.7089\n",
      "Epoch 9/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.5366 - acc: 0.8127 - val_loss: 0.8959 - val_acc: 0.7103- acc: 0.814 - ETA: 4 - ETA: 3s - loss: 0.534 - ETA: 2s - loss: 0 - ETA: 1s - loss: \n",
      "Epoch 10/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.4879 - acc: 0.8312 - val_loss: 0.9063 - val_acc: 0.7123\n",
      "Epoch 11/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.4361 - acc: 0.8493 - val_loss: 0.9473 - val_acc: 0.7081\n",
      "Epoch 12/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.3887 - acc: 0.8648 - val_loss: 0.9560 - val_acc: 0.7191\n",
      "Epoch 13/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.3466 - acc: 0.8803 - val_loss: 1.0607 - val_acc: 0.70351s - loss: 0.344 - ETA: 0s - loss: 0.3452 - ac\n",
      "Epoch 14/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.2961 - acc: 0.9004 - val_loss: 1.0886 - val_acc: 0.7057s: 0.2955 \n",
      "Epoch 15/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.2555 - acc: 0.9135 - val_loss: 1.1030 - val_acc: 0.7179 - acc: 0.9 - ET - ETA: 4s - loss: 0.2521 - acc: 0.915 - ETA: 4s -  - ETA: 2s -  - ETA: 1s - los\n",
      "Epoch 16/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.2152 - acc: 0.9278 - val_loss: 1.1928 - val_acc: 0.7059oss: 0.2142 - a - ETA: 0s - loss: 0.2149 -\n",
      "Epoch 17/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.1773 - acc: 0.9422 - val_loss: 1.2545 - val_acc: 0.7101 0.1739  - ETA: 3s - loss: 0.1\n",
      "Epoch 18/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.1475 - acc: 0.9524 - val_loss: 1.3072 - val_acc: 0.7122 3s - loss: 0.14 - ETA: 2s - loss: 0.1462 - acc: 0.9 - ETA: 2s - loss: 0.14 - ETA: 1s - loss: 0.1 - ETA: 0s - loss: 0.1471 - acc: 0.\n",
      "Epoch 19/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.1208 - acc: 0.9620 - val_loss: 1.4286 - val_acc: 0.7083 5s - lo - ETA: 1s -\n",
      "Epoch 20/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0949 - acc: 0.9716 - val_loss: 1.4847 - val_acc: 0.70858s - loss: 0.0922 - ETA: 7s - loss: 0.0921 - acc: 0.973 - ETA: 7s - lo - ET\n",
      "Epoch 21/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0767 - acc: 0.9774 - val_loss: 1.5757 - val_acc: 0.7017: 0 - ETA: 9s - loss: 0.0727 - acc: 0.9 - ETA: 9s - loss: 0.0730 - a - ETA: 8s - loss: 0.0734 - acc: 0.9 - ETA: 8s - loss: 0.0734 - acc: 0 - ETA: 8s - loss: 0.0732 - acc:  - ETA: 8s - loss: 0.0734 - acc: 0 - ETA: 7s - los - ETA: 6s -  - ETA: 4s - loss: 0.0753 -  - ETA: - ETA: 2s -  - ETA: 0s - loss: 0.0764 - acc: 0.9 - ETA: 0s - loss: 0.0764 - a\n",
      "Epoch 22/100\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.0623 - acc: 0.9827 - val_loss: 1.6338 - val_acc: 0.7108s - loss: 0. - ETA: 0s - loss: 0.0621 - acc: 0.982 - ETA: 0s - loss: 0.0621 - acc:  - ETA: 0s - loss: 0.0624 - acc:\n",
      "Epoch 23/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0486 - acc: 0.9870 - val_loss: 1.7362 - val_acc: 0.70356s - loss: 0.0467 - acc: 0.988 - ETA: 6s - loss: 0.046 - ETA: 5s - loss: 0 - ETA: 4s - loss: 0.0473 - ETA: 1s - loss: - ETA: 0s - loss: 0.0485 - acc: 0.9\n",
      "Epoch 24/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0383 - acc: 0.9904 - val_loss: 1.8084 - val_acc: 0.7075- loss - E - ETA: 12s - loss: 0.03 - ETA - ETA: 10s - loss: 0.0360 - acc: - - ETA: 7s - loss: 0.036 - ETA: 6s - loss: 0.0367 - - ETA: 3s - loss: 0.0374 - a - ETA: 3s - loss: 0.0374  - ETA: 2s - loss: 0. - ETA: 1s - \n",
      "Epoch 25/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0317 - acc: 0.9924 - val_loss: 1.8305 - val_acc: 0.7107 0.0310 - ETA: 5s - loss: 0.0317 - - ETA: 4s - loss: 0.0320 - ac - ETA: 4s - loss: 0.0 - ETA: 0s - loss: 0.0317 - acc: - ETA: 0s - loss: 0.0317 - ac\n",
      "Epoch 26/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0255 - acc: 0.9944 - val_loss: 1.9044 - val_acc: 0.7137.9 - ETA: 1s - loss: 0 - ETA: 0s - loss: 0.0256 - acc: \n",
      "Epoch 27/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0214 - acc: 0.9950 - val_loss: 1.9463 - val_acc: 0.7148oss: 0.0206 -  - ETA: 2s - loss: 0.0205 - acc: 0. - ETA: 1s - loss: 0.0205  - ETA: 1s - loss\n",
      "Epoch 28/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0164 - acc: 0.9969 - val_loss: 2.0360 - val_acc: 0.7128 - ETA: 1s - loss: 0.0166 - acc: 0.996 - ETA: 1\n",
      "Epoch 29/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0154 - acc: 0.9966 - val_loss: 2.0268 - val_acc: 0.7100ss:  - ETA: 6s - loss - ETA:  - ETA: 1s - lo\n",
      "Epoch 30/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.0138 - acc: 0.9971 - val_loss: 2.0866 - val_acc: 0.7139\n",
      "Epoch 31/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0099 - acc: 0.9982 - val_loss: 2.1428 - val_acc: 0.7103- loss:  - ETA: 2s - loss: 0.0094 - ETA: 1s - loss: 0.0096 - acc: 0 - ETA: 1s - loss: 0.0 - ETA: 0s - loss: 0.0098 - acc:\n",
      "Epoch 32/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0088 - acc: 0.9985 - val_loss: 2.1704 - val_acc: 0.7134: 0. - ETA: 5s  - ETA: 3s - loss: 0. - ETA: 2s - ETA: 0s - loss: 0.00\n",
      "Epoch 33/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.0081 - acc: 0.9984 - val_loss: 2.2220 - val_acc: 0.7132\n",
      "Epoch 34/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0059 - acc: 0.9992 - val_loss: 2.2254 - val_acc: 0.7167: 7s - loss: 0.0045 - ETA: 6s - loss: 0.0046 - a - ETA: 3s - loss: 0.0057  - ETA: 0s - loss: 0.005\n",
      "Epoch 35/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0063 - acc: 0.9988 - val_loss: 2.2300 - val_acc: 0.7168s - los - ETA: 7s - loss: 0.0073 - acc: 0 - ETA: 7s - loss: 0.0073\n",
      "Epoch 36/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0069 - acc: 0.9985 - val_loss: 2.3124 - val_acc: 0.70963s - loss: 0.0068  - ETA: - ETA: 0s - loss: 0.0068 - acc: 0.9 - ETA: 0s - loss: 0.0068 - ac\n",
      "Epoch 37/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0052 - acc: 0.9990 - val_loss: 2.3271 - val_acc: 0.7097s - loss: 0.0046 - ETA: 6 - ETA: 5s - loss: 0.0054 - ETA: 4s - loss: 0.0053  - ETA: 3s - loss\n",
      "Epoch 38/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0036 - acc: 0.9995 - val_loss: 2.3540 - val_acc: 0.7121\n",
      "Epoch 39/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0045 - acc: 0.9990 - val_loss: 2.3417 - val_acc: 0.7114TA: 16s - loss: 0.00 - ETA:  - ET - ETA: 5 - ETA: 3s - loss: 0.0044 - ac - ETA: 3s - loss: 0\n",
      "Epoch 40/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0056 - acc: 0.9987 - val_loss: 2.3526 - val_acc: 0.7119s - loss: 0.0044 - - ETA: 20s - loss: 0.0049 - acc: 0.99 - ETA: 20s - loss: 0.0049 - - ETA: 19s - loss: 0.0055 - acc:  - ETA: 19s - loss - ETA: 19s - loss: 0.0061 - acc:  - - ETA: 12s - loss - ETA: 10s - loss: 0. - ETA: 8s - loss: 0.0048 - ac - ETA: 8s - - ETA: 6s - loss: 0.0048 - acc: 0.999 - ETA: 6s - loss: 0.0048 - - ETA: 6s - loss: 0.004 - - ETA: 3s - loss: 0.00\n",
      "Epoch 41/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0031 - acc: 0.9995 - val_loss: 2.3811 - val_acc: 0.7134: 7s - loss: 0 - ETA: 6s - - ETA: 4s - loss: 0.0 - ETA: 1s - loss: 0.0032 - acc: - ETA: 0s - loss: 0\n",
      "Epoch 42/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0033 - acc: 0.9992 - val_loss: 2.3491 - val_acc: 0.7093\n",
      "Epoch 43/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0046 - acc: 0.9991 - val_loss: 2.3842 - val_acc: 0.7122027 - acc: 0.99 - ETA: 8s - loss: 0.0027  - ETA: 8s - loss: 0.0028 - acc: 0.9 - ETA: 8s - loss: 0.0028 - ac - ETA: 7s - ETA: 3s - lo\n",
      "Epoch 44/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0038 - acc: 0.9991 - val_loss: 2.4202 - val_acc: 0.7121\n",
      "Epoch 45/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0020 - acc: 0.9998 - val_loss: 2.4270 - val_acc: 0.7154 - loss: 0.0\n",
      "Epoch 46/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0046 - acc: 0.9989 - val_loss: 2.4587 - val_acc: 0.7104s - loss:  - ETA: 1 - ETA: 9s - lo - ETA: 7s - loss: 0.0047 - acc: 0. - ETA: 7s - loss: 0.0047 - acc: 0.9 - ETA: 7s  - ETA: 5s - loss: 0.0049 - acc: 0.9 - ETA - ETA: 3s - loss: 0.0049 - acc: - ETA: 3s - loss: 0.0048 - ac - ETA: 2s - loss: 0.0048 - acc:  - ETA: 2s - loss: 0.004 - ETA: 1\n",
      "Epoch 47/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0031 - acc: 0.9992 - val_loss: 2.4687 - val_acc: 0.7170s \n",
      "Epoch 48/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0043 - acc: 0.9991 - val_loss: 2.4592 - val_acc: 0.7138: 0.9 - ET\n",
      "Epoch 49/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0030 - acc: 0.9994 - val_loss: 2.4957 - val_acc: 0.7084- - ETA: 1s -\n",
      "Epoch 50/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0017 - acc: 0.9997 - val_loss: 2.4915 - val_acc: 0.7107TA: 8s - loss: 6.3507e-04  - ETA: 7s - loss: 6.6558e-04 - acc: 1. - ETA: 7s - loss: 7.4417e-04 - acc: - ETA: 7s - loss: - ETA: 5s - loss: 0.0011 - acc: 0.9 - ETA: 5 - ETA: \n",
      "Epoch 51/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0024 - acc: 0.9996 - val_loss: 2.5167 - val_acc: 0.7112 0.0027 - acc: 0.9 - ETA: 6s - loss: 0.0027 -  - ETA: 5s - los - \n",
      "Epoch 52/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 2.5174 - val_acc: 0.7063\n",
      "Epoch 53/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 2.5061 - val_acc: 0.7099TA: 7s - loss: 0.00 - ETA: 6s - loss: 0.0024  - ETA: 6s - loss: 0.0027 -  - ETA: 5s - loss: 0.0026 - acc: 0.99 - ETA: 5s - loss: 0.0 - ETA: 4s - loss: 0.00 - ETA: 3s - loss: 0.0027 - acc: 0.  - ETA: 0s - loss: 0.0026 - acc: - ETA: 0s - loss: 0.0025 - ac\n",
      "Epoch 54/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 3.2965e-04 - acc: 1.0000 - val_loss: 2.5291 - val_acc: 0.7126loss: 3. - ETA: 3s - loss: 3.4545e-04 - - ETA: 2s - loss: 3.4066e-04 - a - ETA: 2s - los - ETA: 0s - loss: 3.3075e-04 - acc: 1 - ETA: 0s - loss: 3.3068e-04 - acc: 1\n",
      "Epoch 55/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0023 - acc: 0.9995 - val_loss: 2.5236 - val_acc: 0.7168: 18s - loss: 1.6025e- - ETA: 18s - lo - ETA: 17s  - ETA: 15s - loss: 1.6954e-04 - a - ETA: 14s - loss: 1.69 - E - ETA: 4s - loss: 0.0026 - a - ETA: 4s - - E - ETA: 0s - loss: 0.0024\n",
      "Epoch 56/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 2.5860 - val_acc: 0.7081 1s - loss:\n",
      "Epoch 57/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 8.2835e-04 - acc: 0.9999 - val_loss: 2.5587 - val_acc: 0.7184 6s - ETA: 2s - loss: 8.8477e-04 - acc:  - ETA: 1s - \n",
      "Epoch 58/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 1.5082e-04 - acc: 1.0000 - val_loss: 2.5941 - val_acc: 0.7171ss: 1.5423e-04 - ac - ETA: 9s - lo\n",
      "Epoch 59/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0020 - acc: 0.9995 - val_loss: 2.5850 - val_acc: 0.7157 - ETA: 3s - loss: 0.0\n",
      "Epoch 60/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 6.1826e-04 - acc: 0.9999 - val_loss: 2.6688 - val_acc: 0.7106A: 7s - loss: 1.1588e-04  - ETA: 6s - loss: 1.1598e-04 - - ETA: 5s - loss: 1.1632e-04 - ETA: 4s - loss: 1.1781e-04 - ac - E - ETA: \n",
      "Epoch 61/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0014 - acc: 0.9997 - val_loss: 2.6046 - val_acc: 0.7141\n",
      "Epoch 62/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 1.0776e-04 - acc: 1.0000 - val_loss: 2.6370 - val_acc: 0.7148\n",
      "Epoch 63/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 2.6294 - val_acc: 0.7132- loss - ETA: 17s - lo - ETA: 15s - loss:  - ETA: 10s - loss: 0.0038 - a - ETA - ETA: 3s - ETA: 2s - loss: 0. - ETA: 1s - loss: 0.0024 - acc:  - ETA: 0s - loss: 0.0024\n",
      "Epoch 64/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 1.1311e-04 - acc: 1.0000 - val_loss: 2.6477 - val_acc: 0.7153\n",
      "Epoch 65/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 2.6753 - val_acc: 0.71070 - ETA: 9s - loss: 7.1202e-05 -  - ETA: - ETA: 7s - loss - ETA: 5s - loss: 6.3520e - ETA: 2s - loss: 0.0017 - acc: 0.999 - ETA: 2s - loss: 0.0017 - acc: - ETA: 1s - loss: 0.0017 - acc: 0.9 - ETA: 1s\n",
      "Epoch 66/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 2.6398 - val_acc: 0.7161\n",
      "Epoch 67/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 8.7573e-05 - acc: 1.0000 - val_loss: 2.6611 - val_acc: 0.715605 - acc: 1.0 - ETA: 1s - loss: 8.9256e-05 - acc: 1.0 - ETA: 1s - loss: 8.918 - ETA: 0s - loss: 8.8266e-05 -\n",
      "Epoch 68/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 2.6835 - val_acc: 0.71440s - loss: 0.0022 - acc: 0.9\n",
      "Epoch 69/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 1.1592e-04 - acc: 1.0000 - val_loss: 2.6779 - val_acc: 0.7159 1.3467e-04 -  - ETA: 8s - loss: 1.3305e-04 - acc - E - ETA: 0s - loss: 1.1690e-04 - ac\n",
      "Epoch 70/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 2.6610 - val_acc: 0.7138027 - acc - ET - ET - ETA: 4s - loss: 0. - ETA: 1s - loss: \n",
      "Epoch 71/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 2.6302 - val_acc: 0.7120oss: 0.0023 - acc: 0.\n",
      "Epoch 72/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0036 - acc: 0.9991 - val_loss: 2.6700 - val_acc: 0.7113loss: 5. - ETA: 15s - loss: 6.9786e-04 - acc: \n",
      "Epoch 73/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0031 - acc: 0.9992 - val_loss: 2.6955 - val_acc: 0.7103ss: 0.0025 - acc: 0.9 - ETA: 4s - loss: 0.0025 - acc: 0.9 - ETA: 4s - loss: 0.0025 - ETA: 1s - loss: 0.0024 - acc - ETA: 0s - loss: 0.0\n",
      "Epoch 74/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 2.6863 - val_acc: 0.70870s - loss: 0.0020 - acc: 0.9 - ETA: 0s - loss: 0.0020 - acc: 0.999 - ETA: 0s - loss: 0.0020 - acc: 0.99\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0032 - acc: 0.9993 - val_loss: 2.7139 - val_acc: 0.7054 los - ETA: 3s\n",
      "Epoch 76/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 3.5103e-04 - acc: 1.0000 - val_loss: 2.6967 - val_acc: 0.7129s - loss: 4.0409e-04 -  - ETA: 5s - loss: 3.9556e- - ETA: 4s \n",
      "Epoch 77/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0033 - acc: 0.9992 - val_loss: 2.7121 - val_acc: 0.7068 ETA: 14s - loss - ETA: 13 - E\n",
      "Epoch 78/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 2.7286 - val_acc: 0.7084151e-04 - acc:  - ETA: 4 - ETA: 0s - loss: 0.0025 - acc\n",
      "Epoch 79/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0039 - acc: 0.9989 - val_loss: 2.7450 - val_acc: 0.7072 0.0045 - ac - ETA: 7s - loss: 0.004 - ETA: 7 - ETA: 5s - loss: 0.0040 - acc: 0.9 - ETA: 5s - loss: 0.0040 - acc: 0 - ETA: 4s - los - ETA: 3s - loss: 0.0037 - ETA: 0s - loss: 0.0039 - acc: \n",
      "Epoch 80/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0032 - acc: 0.9991 - val_loss: 2.7431 - val_acc: 0.7058\n",
      "Epoch 81/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0044 - acc: 0.9986 - val_loss: 2.7196 - val_acc: 0.7071ss: 0.0041 - acc: 0. - ETA: 2s - loss: 0.0041 - acc:  - ET\n",
      "Epoch 82/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0036 - acc: 0.9989 - val_loss: 2.7351 - val_acc: 0.7100\n",
      "Epoch 83/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0038 - acc: 0.9989 - val_loss: 2.7320 - val_acc: 0.7079: 8s - loss: 0.0028 - acc: - E - E  - ETA: 1s - loss: 0.0040 - acc: 0.9 - ETA: 1s -\n",
      "Epoch 84/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 4.8951e-04 - acc: 1.0000 - val_loss: 2.7262 - val_acc: 0.7123loss: 6.5374 - ETA: 9s - loss: 6.3018e-04 - acc: - ETA: 6s - loss: 6.0194e-04 - ETA: 5s - loss: 5.8400e - ETA\n",
      "Epoch 85/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 2.0232e-04 - acc: 0.9999 - val_loss: 2.7999 - val_acc: 0.7044  - ETA: 6s - loss: 8. - ETA: 4s - loss: 8.2595e-05 - acc: 1.0 - ETA: 4s - loss: 8.2449e-05 -  - ETA: 1s - loss\n",
      "Epoch 86/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 2.7730 - val_acc: 0.7109\n",
      "Epoch 87/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 1.3612e-04 - acc: 1.0000 - val_loss: 2.7611 - val_acc: 0.71324768e - E - ETA: 3s - loss: 1.4280e-04 - a - ETA: 2s - loss: 1 - ETA: 1s - loss:\n",
      "Epoch 88/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 8.1571e-04 - acc: 0.9997 - val_loss: 2.7767 - val_acc: 0.7038.6345e-05 -  - ETA: 6s - los - ETA\n",
      "Epoch 89/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0038 - acc: 0.9990 - val_loss: 2.7524 - val_acc: 0.7123cc:  - ETA: 2s - loss: 0.0043 - \n",
      "Epoch 90/100\n",
      "1562/1562 [==============================] - 45s 29ms/step - loss: 0.0024 - acc: 0.9991 - val_loss: 2.7649 - val_acc: 0.7099\n",
      "Epoch 91/100\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.0030 - acc: 0.9991 - val_loss: 2.8318 - val_acc: 0.7027A: 1s - loss:\n",
      "Epoch 92/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 2.7839 - val_acc: 0.7109-  - ETA: 8s - - ETA: 7s - loss: 0.0022  - ETA: - ETA: 4s - los - ETA: 3s - loss: \n",
      "Epoch 93/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0016 - acc: 0.9997 - val_loss: 2.7615 - val_acc: 0.7154\n",
      "Epoch 94/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 2.7541 - val_acc: 0.709504 - ETA: 15s - loss - ETA: 12s - \n",
      "Epoch 95/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 2.7726 - val_acc: 0.7089 - ETA: 7s - loss: 3.3573e-04 - ETA: 6s - loss: 4.9298e-04 - acc: - ETA: 5s -  - ETA:\n",
      "Epoch 96/100\n",
      "1562/1562 [==============================] - 26s 16ms/step - loss: 0.0039 - acc: 0.9989 - val_loss: 2.8642 - val_acc: 0.7019\n",
      "Epoch 97/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 2.8049 - val_acc: 0.7099  - ETA: 3s - loss: 0. - ETA: 0s - loss: 0.0018 - acc: - ETA: 0s - loss: 0.0018 - acc: 0.9\n",
      "Epoch 98/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 7.6571e-04 - acc: 0.9998 - val_loss: 2.9005 - val_acc: 0.6985ss: 2.9698e-04 - acc: 1.0 - ETA: 1s - \n",
      "Epoch 99/100\n",
      "1562/1562 [==============================] - 26s 17ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 2.7906 - val_acc: 0.7115\n",
      "Epoch 100/100\n",
      "1562/1562 [==============================] - 25s 16ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 2.8309 - val_acc: 0.7066ETA: 7s - loss: 0.0023 - a -  - ETA: 4s - loss: 0.0020 -  - ETA: 4s - loss: 0. - ETA: 3s - l - ETA: 1s - loss: 0.0018 - acc: 0.9 - ETA: 1s - loss: 0.0018 - ac - ETA: 0s - loss: 0.001\n",
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.7203 - acc: 0.3736 - val_loss: 1.3610 - val_acc: 0.5103 ETA: 1\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.5001 - acc: 0.4614 - val_loss: 1.2991 - val_acc: 0.5355\n",
      "Epoch 3/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.3930 - acc: 0.4995 - val_loss: 1.2154 - val_acc: 0.5720oss: 1.397\n",
      "Epoch 4/100\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 1.3254 - acc: 0.5256 - val_loss: 1.0983 - val_acc: 0.6103\n",
      "Epoch 5/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.2742 - acc: 0.5438 - val_loss: 1.1375 - val_acc: 0.6031 loss: 1 - ETA: 1s - loss: \n",
      "Epoch 6/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.2396 - acc: 0.5594 - val_loss: 1.1560 - val_acc: 0.5995\n",
      "Epoch 7/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.2037 - acc: 0.5725 - val_loss: 1.1297 - val_acc: 0.6073\n",
      "Epoch 8/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.1795 - acc: 0.5819 - val_loss: 1.0938 - val_acc: 0.6239-\n",
      "Epoch 9/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.1593 - acc: 0.5852 - val_loss: 0.9809 - val_acc: 0.6550\n",
      "Epoch 10/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 1.1310 - acc: 0.5981 - val_loss: 1.0133 - val_acc: 0.64491.1332  - ETA: 6s - loss: 1.1312 - acc: 0.5 - ETA: 5s - loss: 1.1303 - ac - ETA: 5s  - ETA: 1s - l\n",
      "Epoch 11/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.1160 - acc: 0.6033 - val_loss: 0.9907 - val_acc: 0.6515\n",
      "Epoch 12/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.0957 - acc: 0.6129 - val_loss: 0.9821 - val_acc: 0.6527\n",
      "Epoch 13/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.0970 - acc: 0.6104 - val_loss: 0.9664 - val_acc: 0.6610\n",
      "Epoch 14/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.0714 - acc: 0.6206 - val_loss: 1.0569 - val_acc: 0.6415: 7s - ETA: 3s - loss: 1.0726 - ETA: 0s - loss: 1.0723 - acc: 0.6\n",
      "Epoch 15/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.0579 - acc: 0.6232 - val_loss: 0.9523 - val_acc: 0.6724\n",
      "Epoch 16/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.0468 - acc: 0.6283 - val_loss: 0.9874 - val_acc: 0.6602-  - ETA: 0s - loss: 1.0475 - acc: 0\n",
      "Epoch 17/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.0355 - acc: 0.6334 - val_loss: 0.9445 - val_acc: 0.6662: 1s - lo - ETA: 0s - loss: 1.0361 - acc:\n",
      "Epoch 18/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.0230 - acc: 0.6377 - val_loss: 0.9296 - val_acc: 0.6824- a - ETA: 0s - loss: 1.0228 \n",
      "Epoch 19/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 1.0110 - acc: 0.6432 - val_loss: 0.9175 - val_acc: 0.6876091 -  - ETA: 8s - loss: 1.0090 -  - ETA: 3s - l - ETA: 1s - loss: 1.0126 - acc: 0 - ETA: 1s - \n",
      "Epoch 20/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.9957 - acc: 0.6499 - val_loss: 0.9602 - val_acc: 0.6722ETA: 0s - loss: 0.9973 - a\n",
      "Epoch 21/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.9969 - acc: 0.6475 - val_loss: 0.8907 - val_acc: 0.6939\n",
      "Epoch 22/100\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.9874 - acc: 0.6509 - val_loss: 0.9594 - val_acc: 0.6760\n",
      "Epoch 23/100\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.9774 - acc: 0.6555 - val_loss: 0.9684 - val_acc: 0.6760\n",
      "Epoch 24/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.9709 - acc: 0.6567 - val_loss: 0.9088 - val_acc: 0.6916\n",
      "Epoch 25/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.9644 - acc: 0.6607 - val_loss: 0.9406 - val_acc: 0.6762\n",
      "Epoch 26/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.9593 - acc: 0.6613 - val_loss: 0.9490 - val_acc: 0.680378 - acc: 0. - ETA: 3s - \n",
      "Epoch 27/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.9461 - acc: 0.6679 - val_loss: 0.9483 - val_acc: 0.6827TA: 6s - loss: 0.9452 - acc - ETA: 5s - loss: 0.9451 - acc: 0.6 - ETA: 5s - loss: 0.9444 - acc: 0. - ETA: 5s - - ETA - ETA: 1 - ETA: 0s - loss: 0.9464 - acc: \n",
      "Epoch 28/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.9364 - acc: 0.6678 - val_loss: 0.8785 - val_acc: 0.7010s: 0. - ETA: 8s - loss: 0.9382 -  - ETA: 6s - loss: 0 - ETA: 5s - loss: 0.9389 - ac\n",
      "Epoch 29/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.9369 - acc: 0.6712 - val_loss: 0.8984 - val_acc: 0.6905\n",
      "Epoch 30/100\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 0.9260 - acc: 0.6745 - val_loss: 0.8983 - val_acc: 0.6905 acc: 0. - ETA: 8s - loss: 0. - ETA: 7s - l - ET\n",
      "Epoch 31/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.9186 - acc: 0.6759 - val_loss: 0.8449 - val_acc: 0.70948 - ETA: 3s - loss: - ETA - ETA: 0s - loss: 0.9178 - acc: 0.67 - ETA: 0s - loss: 0.9183 - acc: 0.67 - ETA: 0s - loss: 0.9181 - acc\n",
      "Epoch 32/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.9125 - acc: 0.6788 - val_loss: 0.8980 - val_acc: 0.6987oss: 0.9158 - ETA: 3s - - ETA: 1s - loss: 0.9142 - acc: 0.678 - ETA: 1s - loss: 0.91 - ETA: 0s - loss: 0.913\n",
      "Epoch 33/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.9118 - acc: 0.6798 - val_loss: 0.8946 - val_acc: 0.6991\n",
      "Epoch 34/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.9000 - acc: 0.6822 - val_loss: 0.9486 - val_acc: 0.6904ETA: 4s - loss: 0.902  - ETA: 1s - lo\n",
      "Epoch 35/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8989 - acc: 0.6828 - val_loss: 0.8697 - val_acc: 0.7059\n",
      "Epoch 36/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8961 - acc: 0.6846 - val_loss: 0.9164 - val_acc: 0.6942: 0.9005 - acc: 0.68 - ETA: 10s  - ETA: 9s - loss: 0.900 - ETA: 6s - loss: 0.9023 - acc: 0.6 - ETA: 6s - loss: 0.9023 - acc: - ETA: 5s -\n",
      "Epoch 37/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8881 - acc: 0.6874 - val_loss: 0.8293 - val_acc: 0.7167\n",
      "Epoch 38/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8797 - acc: 0.6887 - val_loss: 0.8425 - val_acc: 0.7103- loss: 0.8783 - acc: - ETA: 0s - loss: 0.8\n",
      "Epoch 39/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8811 - acc: 0.6906 - val_loss: 0.8545 - val_acc: 0.7183s - loss: 0.8818  - ETA: 2s - loss: - ETA: 1s - loss: 0.\n",
      "Epoch 40/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8746 - acc: 0.6916 - val_loss: 0.9936 - val_acc: 0.6642\n",
      "Epoch 41/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8740 - acc: 0.6915 - val_loss: 0.7838 - val_acc: 0.7275- acc: 0.691 - ETA: 0s - loss: 0.8734 - acc: 0.\n",
      "Epoch 42/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8616 - acc: 0.6978 - val_loss: 0.8476 - val_acc: 0.7154\n",
      "Epoch 43/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8594 - acc: 0.6980 - val_loss: 0.9317 - val_acc: 0.6914A: 0s - loss: 0.8588 - acc\n",
      "Epoch 44/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8565 - acc: 0.6965 - val_loss: 0.8408 - val_acc: 0.7184loss: - ETA: 4s - loss: 0.8557 - acc: 0\n",
      "Epoch 45/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8574 - acc: 0.6972 - val_loss: 0.8366 - val_acc: 0.7136\n",
      "Epoch 46/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8534 - acc: 0.6983 - val_loss: 0.8484 - val_acc: 0.7191\n",
      "Epoch 47/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8482 - acc: 0.7029 - val_loss: 0.8810 - val_acc: 0.7100: 5s - loss: 0.8 - ETA: 4s - lo - ETA: 2s - loss: 0.8495 - acc: 0.70 - ETA: 2s - loss: 0.8496 - acc: - ETA: 2s - loss: 0.8499 - ac - ET\n",
      "Epoch 48/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8466 - acc: 0.7004 - val_loss: 0.8648 - val_acc: 0.7095 1s - l - ETA: 0s - loss: 0.8460 - acc: 0\n",
      "Epoch 49/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8418 - acc: 0.7028 - val_loss: 0.8502 - val_acc: 0.7106\n",
      "Epoch 50/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8439 - acc: 0.7033 - val_loss: 0.7684 - val_acc: 0.7363\n",
      "Epoch 51/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8352 - acc: 0.7059 - val_loss: 0.8369 - val_acc: 0.7213\n",
      "Epoch 52/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8285 - acc: 0.7088 - val_loss: 0.8207 - val_acc: 0.7214\n",
      "Epoch 53/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8264 - acc: 0.7072 - val_loss: 0.8444 - val_acc: 0.7222 - loss: 0.8194 - ac - ETA: 7s - ETA: 5s - loss\n",
      "Epoch 54/100\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8268 - acc: 0.7118 - val_loss: 0.7798 - val_acc: 0.7341825 - ETA: 1s  - ETA: 0s - loss: 0.8271 - acc: 0\n",
      "Epoch 55/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8210 - acc: 0.7111 - val_loss: 0.8231 - val_acc: 0.7242\n",
      "Epoch 56/100\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 0.8250 - acc: 0.7101 - val_loss: 0.8356 - val_acc: 0.7201: 1s - \n",
      "Epoch 57/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8201 - acc: 0.7134 - val_loss: 0.8072 - val_acc: 0.7262\n",
      "Epoch 58/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8120 - acc: 0.7115 - val_loss: 0.8519 - val_acc: 0.7134ss: 0.8095 - acc:  - ETA: 5s - los - ETA: 4s - loss: 0.8103  - ETA: 3s - loss: 0\n",
      "Epoch 59/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8111 - acc: 0.7174 - val_loss: 0.8324 - val_acc: 0.7228\n",
      "Epoch 60/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8062 - acc: 0.7153 - val_loss: 0.7979 - val_acc: 0.7275cc - ETA: 9s - loss:  - ETA: 6s - - ETA: 4s - loss: 0 - ETA: 3s - ETA\n",
      "Epoch 61/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8073 - acc: 0.7176 - val_loss: 0.8282 - val_acc: 0.7236 loss: 0.8058 - acc: 0 - ETA: 1s - loss: 0.8066 - acc: - ETA: 0s - loss: 0.806\n",
      "Epoch 62/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8018 - acc: 0.7167 - val_loss: 0.7743 - val_acc: 0.7382\n",
      "Epoch 63/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.8002 - acc: 0.7182 - val_loss: 0.9185 - val_acc: 0.70240.7966 - - ETA: 10s - loss: 0.7973 - acc:  - ETA - E - ETA: 1s - loss: 0.8\n",
      "Epoch 64/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.8028 - acc: 0.7173 - val_loss: 0.7910 - val_acc: 0.7359\n",
      "Epoch 65/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7985 - acc: 0.7213 - val_loss: 0.8405 - val_acc: 0.7193 ETA: 10s - loss: 0.7975 - - ETA: 9s - loss: 0.7984 - acc: 0.72 -  - ETA: 2s - ETA: 1s - l\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7955 - acc: 0.7212 - val_loss: 0.8048 - val_acc: 0.7306\n",
      "Epoch 67/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.7976 - acc: 0.7193 - val_loss: 0.7647 - val_acc: 0.7414: 5s - ETA: 4s -  - ETA: 2s - loss: 0.7974 - ac - ETA: 2s - ETA: 0s - loss: 0.7971 - acc:\n",
      "Epoch 68/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.7931 - acc: 0.7223 - val_loss: 0.8265 - val_acc: 0.7221 - loss: 0.7922 - acc: 0 - ETA: 4s - lo - ETA: 2s -  - ETA: 1s - lo\n",
      "Epoch 69/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.7941 - acc: 0.7219 - val_loss: 0.7550 - val_acc: 0.7466 ETA: 4s - loss: 0.7965 - ac -\n",
      "Epoch 70/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.7877 - acc: 0.7231 - val_loss: 0.8204 - val_acc: 0.7207TA: 1s - los\n",
      "Epoch 71/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.7878 - acc: 0.7237 - val_loss: 0.8005 - val_acc: 0.7332\n",
      "Epoch 72/100\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.7871 - acc: 0.7239 - val_loss: 0.8312 - val_acc: 0.7210 0.7856 - acc:  - ETA: 3s - loss: 0. - ETA: 2s - loss: 0.7842 - acc: 0.72 - ETA: \n",
      "Epoch 73/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7831 - acc: 0.7249 - val_loss: 0.8367 - val_acc: 0.7258\n",
      "Epoch 74/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7788 - acc: 0.7266 - val_loss: 0.7905 - val_acc: 0.7323: 6s - loss: 0.777 - ETA: 5s - loss: 0.7784 - acc: - ETA: 5s - loss: 0.7792 - acc: 0.7 - ETA: 4s - loss: 0.7782 - acc: 0.72 - ETA: 4s - loss: - ETA: 1s - loss: 0.7788 - - ETA: 0s - loss: 0.7785 \n",
      "Epoch 75/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7783 - acc: 0.7276 - val_loss: 0.8102 - val_acc: 0.73087801 - acc: 0. - ETA: 8 -  - ETA: 0s - loss: 0.7781 - acc: 0.727 - ETA: 0s - loss: 0.7785 - acc:\n",
      "Epoch 76/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7764 - acc: 0.7274 - val_loss: 0.7661 - val_acc: 0.7454\n",
      "Epoch 77/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.7757 - acc: 0.7284 - val_loss: 0.8227 - val_acc: 0.7243\n",
      "Epoch 78/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7694 - acc: 0.7282 - val_loss: 0.7880 - val_acc: 0.7335- loss: 0.7655 - acc - ETA: 4\n",
      "Epoch 79/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.7712 - acc: 0.7286 - val_loss: 0.7375 - val_acc: 0.7498A: 8s - loss: 0.774 - ETA: 7s - loss: 0. - ETA: 6s - loss: 0.7730 - ETA: 5s - loss:  - ETA: 2s - loss: 0.773 - ETA: 1s \n",
      "Epoch 80/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7666 - acc: 0.7307 - val_loss: 0.7785 - val_acc: 0.7377: 7s - loss: 0.7582 - ac - ETA - E - ETA: 3s - loss: 0.7626 -  - ETA: 0s - loss: 0.7675 - ac\n",
      "Epoch 81/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7662 - acc: 0.7311 - val_loss: 0.8001 - val_acc: 0.7381\n",
      "Epoch 82/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7685 - acc: 0.7300 - val_loss: 0.7394 - val_acc: 0.7514s: 0 - ETA: 0s - loss: 0.7678 - acc: \n",
      "Epoch 83/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.7661 - acc: 0.7305 - val_loss: 0.7420 - val_acc: 0.7524\n",
      "Epoch 84/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.7612 - acc: 0.7311 - val_loss: 0.7945 - val_acc: 0.7418s - ETA: 0s - loss: 0.76\n",
      "Epoch 85/100\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.7590 - acc: 0.7344 - val_loss: 0.7692 - val_acc: 0.7420: 0.7 \n",
      "Epoch 86/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7582 - acc: 0.7315 - val_loss: 0.7971 - val_acc: 0.73580.730 - ETA: 2s - loss: 0.7604 - acc: 0.7 - ETA: 2s - loss: 0. - ETA: 1s - loss: 0.7597 - acc: 0.73 - ETA: \n",
      "Epoch 87/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7586 - acc: 0.7346 - val_loss: 0.8213 - val_acc: 0.72872s - ETA: 0s - loss: 0.75\n",
      "Epoch 88/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7561 - acc: 0.7353 - val_loss: 0.8116 - val_acc: 0.7369loss: 0.7561 - a - ETA: 1s - loss: 0\n",
      "Epoch 89/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7571 - acc: 0.7335 - val_loss: 0.8135 - val_acc: 0.7374- ETA: - ETA: 0s - loss: 0.7569 - acc: 0.7\n",
      "Epoch 90/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7487 - acc: 0.7372 - val_loss: 0.7202 - val_acc: 0.7581474 - ETA: \n",
      "Epoch 91/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7534 - acc: 0.7335 - val_loss: 0.8076 - val_acc: 0.73227 - ETA: 7s - loss:  - ETA: 5s - loss: 0.7534 - acc: 0.7 - E - ETA: 3s - loss: 0.7540 - - ETA:  - ETA: 1s - loss:\n",
      "Epoch 92/100\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 0.7532 - acc: 0.7363 - val_loss: 0.7671 - val_acc: 0.7412TA: 3s - loss: 0.7521  - ETA: 0s - loss: 0.7532 - acc\n",
      "Epoch 93/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7500 - acc: 0.7359 - val_loss: 0.7792 - val_acc: 0.7420\n",
      "Epoch 94/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7517 - acc: 0.7371 - val_loss: 0.7688 - val_acc: 0.7416: 4s - los - ETA: 1s - loss: 0\n",
      "Epoch 95/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7484 - acc: 0.7376 - val_loss: 0.7470 - val_acc: 0.7477- l - - ETA: 7s - loss: 0.74 - ETA: 6s -  - ETA: 4s - loss: 0.7 - ETA: 3s - loss: 0.7517 -  - ETA: 3 - ETA: 1s - lo\n",
      "Epoch 96/100\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 0.7485 - acc: 0.7392 - val_loss: 0.7896 - val_acc: 0.7385s - loss:\n",
      "Epoch 97/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7426 - acc: 0.7382 - val_loss: 0.7618 - val_acc: 0.7439: - ETA: 6s - loss: 0.7379 - ETA: 6s\n",
      "Epoch 98/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7406 - acc: 0.7403 - val_loss: 0.7401 - val_acc: 0.7560 \n",
      "Epoch 99/100\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 0.7468 - acc: 0.7378 - val_loss: 0.7488 - val_acc: 0.7510\n",
      "Epoch 100/100\n",
      "1562/1562 [==============================] - 29s 19ms/step - loss: 0.7468 - acc: 0.7360 - val_loss: 0.7649 - val_acc: 0.7487s - loss: - ETA: 9s - loss: 0.7476 - a\n",
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 1.6044 - acc: 0.4634 - val_loss: 1.2048 - val_acc: 0.58511.6064 - acc: 0\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 1.3255 - acc: 0.5896 - val_loss: 1.0414 - val_acc: 0.6503\n",
      "Epoch 3/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 1.2072 - acc: 0.6449 - val_loss: 0.9776 - val_acc: 0.6705: 0.640 - ETA: 7s - loss: 1.219 - ETA: 7s - - ETA: 3s - loss: 1.2099 - - ETA: 2s  - ETA: 0s - loss: 1.2069 - acc: 0.6 - ETA: 0s - loss: 1.2074 \n",
      "Epoch 4/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 1.1360 - acc: 0.6754 - val_loss: 0.9289 - val_acc: 0.6925 acc: 0.6 - ETA: 7s - loss: 1.1429 - acc: - ETA: 6s - loss - ETA: 5s - loss: 1.1436 -  - ETA: 4s - loss: 1.1420 - acc: 0.6 - ETA: 4s -  - ETA: 3s - loss: 1.1398 - acc: 0.673 - ETA: 3s - loss: 1.1\n",
      "Epoch 5/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 1.0854 - acc: 0.6996 - val_loss: 0.9136 - val_acc: 0.6932 4s - loss: 1.0860 -  - ETA: 3s - loss: 1. - ETA: 2s - loss: 1.0863 - ac - ETA: 2s - loss: 1.0872 - acc - ETA: \n",
      "Epoch 6/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 1.0405 - acc: 0.7191 - val_loss: 0.8773 - val_acc: 0.7060- ETA: 7s - lo - ETA: 6s - loss: - ETA: 2s - loss: 1.0 - ETA: 1s - loss: 1.04 - ETA: 1s - loss: 1.0421 - acc: 0 - ETA: 0s - loss: 1.0411 \n",
      "Epoch 7/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.9951 - acc: 0.7403 - val_loss: 0.8723 - val_acc: 0.7066- loss: 0.9\n",
      "Epoch 8/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.9593 - acc: 0.7526 - val_loss: 0.8440 - val_acc: 0.72310.9643 - acc: 0.752 - ETA: 7s - l - ETA: 5s - loss: 0.9612  - ETA: 5s - loss - ETA: 3s \n",
      "Epoch 9/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.9133 - acc: 0.7759 - val_loss: 0.8339 - val_acc: 0.7246\n",
      "Epoch 10/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.8766 - acc: 0.7920 - val_loss: 0.8677 - val_acc: 0.7125ETA: 2s - los - ETA: 0s - loss: 0.8753 - acc: 0. - ETA: 0s - loss: 0.8762 - acc: 0. - ETA: 0s - loss: 0.8761 - acc\n",
      "Epoch 11/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.8450 - acc: 0.8082 - val_loss: 0.8406 - val_acc: 0.7274 - loss: 0.8465 - acc - ETA: 6s - loss: 0.8470 - acc: 0.8 - ETA: 6s - ETA: 2s - loss: 0.8450 - acc: 0.809 \n",
      "Epoch 12/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.8153 - acc: 0.8220 - val_loss: 0.8596 - val_acc: 0.7160152 - acc:  - \n",
      "Epoch 13/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.7885 - acc: 0.8366 - val_loss: 0.8542 - val_acc: 0.7235\n",
      "Epoch 14/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.7570 - acc: 0.8484 - val_loss: 0.8769 - val_acc: 0.7218- ETA: 8s - loss: 0.7578 - a - ETA: 5s - l - ETA: 4s - loss - ETA: 2s - loss: 0.7560 -  - ETA: 0s - loss: 0.7569 - acc: 0.84\n",
      "Epoch 15/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.7311 - acc: 0.8610 - val_loss: 0.8682 - val_acc: 0.7233- loss: 0\n",
      "Epoch 16/100\n",
      "1562/1562 [==============================] - 29s 18ms/step - loss: 0.7113 - acc: 0.8702 - val_loss: 0.8733 - val_acc: 0.7216A: 1s -  - ETA: 0s - loss: 0.7113 - acc: 0.870\n",
      "Epoch 17/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6903 - acc: 0.8813 - val_loss: 0.8783 - val_acc: 0.72387s - loss: 0.69 - ETA: 6s - loss: 0.6886 - acc - ETA: 6s - loss: 0.68 - ETA: 5s - lo - ETA: 3s - loss: 0.6895 - acc: 0 - ETA: 3s - loss: - ETA: 2s - loss: 0.6900 - acc: - ETA: 1s - loss: 0.6890 - acc: 0.881 - ETA: 1s - loss: 0.68 - ETA: 0s - loss: 0.689\n",
      "Epoch 18/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6721 - acc: 0.8907 - val_loss: 0.8996 - val_acc: 0.7212cc: 0.88 - ETA: 10s - loss: 0.6737 - a - ETA: 10s  - ETA: 6s - - ETA: 4s  - ETA: 3s - loss: 0.6725 - acc: 0 - ETA: 3s - loss: 0.671 -\n",
      "Epoch 19/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6634 - acc: 0.8953 - val_loss: 0.9118 - val_acc: 0.719321s - loss: 0.65 - ETA: 21s - loss: 0.6516 - - ETA: 21s  - ETA: 20s - loss: 0.6548 - acc: 0.90 - ETA: 20s - loss - ETA: 19s - loss: 0.6562 - acc - ETA: 19s - loss: 0.6575 - - ET - ETA: 2s - loss - ETA: 1s - loss:\n",
      "Epoch 20/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6423 - acc: 0.9033 - val_loss: 0.9239 - val_acc: 0.71782  - ETA: 1s - loss: 0.6419 - acc: 0.9 - ETA: 0s - loss: 0.6420 - acc:  - ETA: 0s - loss: 0.6419 - \n",
      "Epoch 21/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6305 - acc: 0.9097 - val_loss: 0.9012 - val_acc: 0.7237\n",
      "Epoch 22/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6139 - acc: 0.9156 - val_loss: 0.9286 - val_acc: 0.71770.6152 - acc: 0.915 - ET - ETA: 7s - loss: 0.6158 - acc: 0.9 - ETA: 7s - loss: 0.6158 - acc: - ETA: 4s - loss: 0.6136 - acc: 0.9 - ETA: 4s - loss: 0.6130 - acc: 0.9 - \n",
      "Epoch 23/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.6059 - acc: 0.9203 - val_loss: 0.9223 - val_acc: 0.7244\n",
      "Epoch 24/100\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5947 - acc: 0.9249 - val_loss: 0.9357 - val_acc: 0.71699s - loss: 0.5925 -  - ETA: 8s - loss: 0.5922 - ETA: 7s - loss: 0.5916 - acc: 0 - ETA: 7s - loss: 0.5919 - acc: 0.926 - ETA: 7s - loss: 0.5919 - ac - ETA: 5s - l - ETA: 1s - loss: 0.5949 - acc: 0 - ETA: 1s - loss: 0.5956 - acc: 0. - ETA: 1s - loss: 0.5951 - acc - ETA: 0s - loss: 0.5949 - \n",
      "Epoch 25/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5944 - acc: 0.9239 - val_loss: 0.9620 - val_acc: 0.7112955 - acc: 0.9 - ETA: 7s - - ETA: 1s - loss: 0.\n",
      "Epoch 26/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5823 - acc: 0.9291 - val_loss: 0.9491 - val_acc: 0.715610s - loss: 0.5820 - acc: 0.92 - ETA: 10s -  - ETA - \n",
      "Epoch 27/100\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.5737 - acc: 0.9307 - val_loss: 0.9360 - val_acc: 0.7233\n",
      "Epoch 28/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5727 - acc: 0.9334 - val_loss: 0.9665 - val_acc: 0.71290.5707 - acc: 0. - ETA: 14s  - ETA: 13s - lo - E - E - ETA: 9s - loss: 0.57 - ETA: 8s - - ETA:  - ETA: 3s - loss: 0.57\n",
      "Epoch 29/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5637 - acc: 0.9356 - val_loss: 0.9558 - val_acc: 0.7151\n",
      "Epoch 30/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5513 - acc: 0.9398 - val_loss: 0.9665 - val_acc: 0.71713s - loss: 0.54 - ETA: 2s - loss: 0.5486 - ETA: 1s - loss: 0.5500 - acc:  - ETA: 0s - loss: 0.5504 - acc: 0 - ETA: 0s - loss: 0.5509 - a\n",
      "Epoch 31/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5540 - acc: 0.9370 - val_loss: 0.9546 - val_acc: 0.7137s - ETA: 1s - loss\n",
      "Epoch 32/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5534 - acc: 0.9387 - val_loss: 0.9768 - val_acc: 0.7106 0.5533 - acc: 0 - ETA: 1s - loss: 0.5528 - acc:  - ETA: 1s - \n",
      "Epoch 33/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5471 - acc: 0.9409 - val_loss: 0.9561 - val_acc: 0.7135 - loss: 0.5466 - acc - ETA: 5s - l - ETA: 3s - los - ETA: 2s - loss: - ETA: 1s - loss: 0.5470 - acc:  - ETA: 0s - loss: 0.546\n",
      "Epoch 34/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5488 - acc: 0.9405 - val_loss: 0.9590 - val_acc: 0.7111 loss\n",
      "Epoch 35/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5396 - acc: 0.9424 - val_loss: 0.9673 - val_acc: 0.71360.53\n",
      "Epoch 36/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5427 - acc: 0.9407 - val_loss: 0.9686 - val_acc: 0.7091428 - a\n",
      "Epoch 37/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5390 - acc: 0.9435 - val_loss: 0.9631 - val_acc: 0.7182\n",
      "Epoch 38/100\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5331 - acc: 0.9455 - val_loss: 0.9646 - val_acc: 0.7121- loss: 0.5353 - acc:  - ETA: 6s - - ETA: 5s - loss: 0.5335 - a - ETA: 4s - loss: 0.5339 - acc:  - ETA: 4s - loss: 0.5338 - ac - ETA: 3s - loss: 0.5334 -  - ETA: 3s - loss: 0.5332 - acc: 0.9 - ETA: 2s - ETA: 1s - loss: 0.5 - ETA: 0s - loss: 0.5331 - acc: 0\n",
      "Epoch 39/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5270 - acc: 0.9439 - val_loss: 0.9987 - val_acc: 0.7097- loss: 0.5248 - acc: 0.9 - ETA: 5s - loss: 0.5249 - ac - ETA: 2s - loss: 0.5257  - ETA: 1s - loss: 0.5257 - acc: - ETA: 1s - loss\n",
      "Epoch 40/100\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.5277 - acc: 0.9447 - val_loss: 0.9616 - val_acc: 0.71234s - loss: 0.5286 - acc: 0.943 - ETA: 4s - loss: 0. - ETA: 3s - loss: 0.5281 - acc: 0.9 - ETA: 3s - loss: 0.5281 - ETA: 2s - loss: 0.5275 - ETA: 1s - loss: 0.5279 - acc: 0. - ETA: \n",
      "Epoch 41/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5273 - acc: 0.9455 - val_loss: 0.9631 - val_acc: 0.7175s - loss: - ETA: 6s - loss: 0 - ETA: 5s - lo - ETA: 4s - loss: 0.5260 - acc: 0 - ETA: 3s - loss:  - ETA: 0s - loss: 0.5276 - ac\n",
      "Epoch 42/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5248 - acc: 0.9466 - val_loss: 0.9746 - val_acc: 0.7122ss - ETA: 5s - lo - ETA: 3s - lo - ETA: 2s - lo - ETA: 1s - loss: 0.5251 - acc: 0.9 - ETA: 0s - loss: 0.\n",
      "Epoch 43/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5185 - acc: 0.9469 - val_loss: 0.9737 - val_acc: 0.7068 - loss: 0.5185 - acc: 0.\n",
      "Epoch 44/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5219 - acc: 0.9459 - val_loss: 0.9728 - val_acc: 0.7080: 2s - loss: 0. - ETA: 1s - loss: 0.5216 - acc: 0.945 - ETA: 1s - loss: 0.5 - ETA: 0s - loss: 0.5220 - acc: \n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5182 - acc: 0.9477 - val_loss: 0.9586 - val_acc: 0.7117ss: 0.5176 - acc:  - ETA - ETA: 11 - ETA: 4s - loss: 0.5180 - acc: 0. - ETA:  - ETA: 2s - ETA: 0s - loss: 0.5181 - acc:\n",
      "Epoch 46/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5166 - acc: 0.9476 - val_loss: 0.9591 - val_acc: 0.7114cc:  - ETA: 7s - loss:  - ETA: 4s - loss: 0.5158 - acc: - ETA\n",
      "Epoch 47/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5111 - acc: 0.9474 - val_loss: 0.9748 - val_acc: 0.71176s - loss: 0.5097 - acc:  - ETA: 6s - loss: 0. - ETA: 5s - loss: 0.5102 - acc: 0 - ETA: 4s - loss: 0 - ETA: 3s -  - ETA: 2s - loss: 0.5101 - acc: 0.947 \n",
      "Epoch 48/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5155 - acc: 0.9470 - val_loss: 0.9865 - val_acc: 0.7042\n",
      "Epoch 49/100\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 0.5102 - acc: 0.9488 - val_loss: 0.9627 - val_acc: 0.7087: 6s - loss: 0.5090 - acc: 0.94 - ETA: 6s - loss: 0.5090 - acc: 0.9 - ETA: 6s - loss: 0.5094 - ETA: 3s - loss: 0.51 - ETA: 2s - loss: 0.51\n",
      "Epoch 50/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5084 - acc: 0.9479 - val_loss: 0.9531 - val_acc: 0.71610.5100 - - ETA: 1s - loss - ETA: 0s - loss: 0.5088 - \n",
      "Epoch 51/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5113 - acc: 0.9482 - val_loss: 0.9657 - val_acc: 0.7086\n",
      "Epoch 52/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5059 - acc: 0.9515 - val_loss: 0.9768 - val_acc: 0.7083\n",
      "Epoch 53/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5137 - acc: 0.9492 - val_loss: 0.9618 - val_acc: 0.7080A: 6s - loss: 0.5122 - - ET - ETA: 4s - loss: 0.513 - ETA: 3s - loss: 0.5129  - ETA: 0s - loss: 0.5135 - acc: 0.949\n",
      "Epoch 54/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5076 - acc: 0.9490 - val_loss: 0.9661 - val_acc: 0.7121\n",
      "Epoch 55/100\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.5040 - acc: 0.9501 - val_loss: 0.9684 - val_acc: 0.7112\n",
      "Epoch 56/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5046 - acc: 0.9492 - val_loss: 0.9761 - val_acc: 0.7139loss: 0.5052 - - ETA: 0s - loss: 0.5050 - a - ETA: 0s - loss: 0.5047 - acc: 0.\n",
      "Epoch 57/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5005 - acc: 0.9499 - val_loss: 0.9562 - val_acc: 0.7161 loss: 0.5005 - acc: 0.949\n",
      "Epoch 58/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.5024 - acc: 0.9510 - val_loss: 0.9643 - val_acc: 0.7144\n",
      "Epoch 59/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4998 - acc: 0.9519 - val_loss: 1.0059 - val_acc: 0.70626s - loss: 0.4999 - acc: 0.9 - ETA: 6s - loss: 0.4999 - acc: 0.951 \n",
      "Epoch 60/100\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.5047 - acc: 0.9492 - val_loss: 0.9843 - val_acc: 0.7067\n",
      "Epoch 61/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4982 - acc: 0.9510 - val_loss: 0.9738 - val_acc: 0.7128- acc: 0.95 - ETA: 7s - loss: 0.4970 - acc:  - ETA: 7s - loss: 0.4964 - acc: - ETA: 6s - loss: 0.496 - ETA: 1s - loss:  - ETA: 0s - loss: 0.4979 - acc: \n",
      "Epoch 62/100\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.5011 - acc: 0.9508 - val_loss: 0.9692 - val_acc: 0.7087\n",
      "Epoch 63/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4935 - acc: 0.9517 - val_loss: 0.9577 - val_acc: 0.7132.951\n",
      "Epoch 64/100\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.4992 - acc: 0.9498 - val_loss: 0.9836 - val_acc: 0.7138s - loss: 0.4986 -  - ETA: 0s - loss: 0.498 - ETA: 0s - loss: 0.4992 - acc: 0.949\n",
      "Epoch 65/100\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.4964 - acc: 0.9510 - val_loss: 0.9690 - val_acc: 0.7127TA: 0s - loss: 0.4971 -\n",
      "Epoch 66/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4918 - acc: 0.9516 - val_loss: 0.9664 - val_acc: 0.7091s - loss: - ETA: 5s - loss: 0.4909 - a - ETA: 4s - loss: 0.4911 - acc: 0. - ETA: 4s - loss: 0.4913  - ETA: 3s - loss: 0.4908 - acc: 0.952 - ETA: 3s - loss:\n",
      "Epoch 67/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4974 - acc: 0.9508 - val_loss: 0.9688 - val_acc: 0.70925s - loss: 0.4953 - acc: 0.950 - ETA: 5s - loss: 0.4956 - acc - ET - ETA: 2s - loss: 0.4958 - acc:  - ETA: 0s - loss: 0.4977 - acc: 0.950 - ETA: 0s - loss: 0.4974 - acc: 0.950\n",
      "Epoch 68/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4908 - acc: 0.9517 - val_loss: 0.9666 - val_acc: 0.70928s - loss: - ETA: 7s - loss: 0.4930 - acc: 0.9 - ETA: 7s - loss: - ETA: 3s - loss: 0.4909 - acc: 0.951 - ETA: \n",
      "Epoch 69/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4974 - acc: 0.9505 - val_loss: 0.9623 - val_acc: 0.7062A: 0s - loss: 0.4974 - acc: 0.950\n",
      "Epoch 70/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4910 - acc: 0.9539 - val_loss: 0.9553 - val_acc: 0.7123\n",
      "Epoch 71/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4927 - acc: 0.9519 - val_loss: 0.9671 - val_acc: 0.7097 - loss: 0.4929 - acc: 0.9 - ETA: 1s \n",
      "Epoch 72/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4920 - acc: 0.9508 - val_loss: 0.9540 - val_acc: 0.7109\n",
      "Epoch 73/100\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.4944 - acc: 0.9513 - val_loss: 0.9506 - val_acc: 0.7087- ETA: 0s - loss: 0.4947 - acc: 0.951 - ETA: 0s - loss: 0.4947 - acc\n",
      "Epoch 74/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4923 - acc: 0.9532 - val_loss: 0.9772 - val_acc: 0.7036cc: 0 - ETA: 1s -\n",
      "Epoch 75/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4860 - acc: 0.9531 - val_loss: 0.9615 - val_acc: 0.7125\n",
      "Epoch 76/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4844 - acc: 0.9536 - val_loss: 0.9493 - val_acc: 0.7162 - ETA: 7s - los - ETA: 4s - loss: 0.4836 - acc: 0.953 - ETA: 4s - loss: 0.4835 - acc: 0.95 - ETA:  - ETA: 2s - loss: 0.4840 - acc: 0.9 - E\n",
      "Epoch 77/100\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.4885 - acc: 0.9534 - val_loss: 0.9698 - val_acc: 0.7115 9s - loss: 0.4908 - - ETA: 8s - loss: 0.4906 - ETA: 7s - loss: 0.4894 - ac - ETA: 7s - loss: 0.4895 - ac - ETA: 4s - l - ETA: 2s - lo - ETA: 1s - loss: 0.4890 - acc: 0. - ETA: 1s - loss: 0.4892 - acc: 0.9 - ETA: 1s - loss: 0.4893 - acc: 0.9 - ETA: 0s - loss: 0.4891 - acc: 0 - ETA: 0s - loss: 0.4894 \n",
      "Epoch 78/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4846 - acc: 0.9530 - val_loss: 0.9675 - val_acc: 0.7095ETA: 2s - loss: 0.4845 -  - \n",
      "Epoch 79/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4849 - acc: 0.9527 - val_loss: 0.9849 - val_acc: 0.7121loss:  - ETA: 12 - ETA: 7s -  - ETA: 5s - loss: 0.4838 - ac - ETA: 5s - lo - ETA: 1s - loss - ETA: 0s - loss: 0.4848 - a\n",
      "Epoch 80/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4848 - acc: 0.9536 - val_loss: 0.9606 - val_acc: 0.7110\n",
      "Epoch 81/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4883 - acc: 0.9534 - val_loss: 0.9568 - val_acc: 0.7147\n",
      "Epoch 82/100\n",
      "1562/1562 [==============================] - 30s 19ms/step - loss: 0.4847 - acc: 0.9540 - val_loss: 0.9711 - val_acc: 0.7098: 0.4843 - ETA: 2s - loss: 0.4846 - acc: - ETA: 1s - loss: 0.4844 - acc: 0.9 - ETA: 1s \n",
      "Epoch 83/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4888 - acc: 0.9529 - val_loss: 0.9762 - val_acc: 0.7089 0.4890 - a - \n",
      "Epoch 84/100\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.4880 - acc: 0.9529 - val_loss: 0.9600 - val_acc: 0.7115\n",
      "Epoch 85/100\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.4827 - acc: 0.9541 - val_loss: 0.9651 - val_acc: 0.7157954 - ETA: 5s - loss: 0.4 - ETA:  - ETA: 2s - loss: 0.4833 - acc: 0 - ETA: 2s - - ETA: 0s - loss: 0.48\n",
      "Epoch 86/100\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.4859 - acc: 0.9531 - val_loss: 0.9490 - val_acc: 0.7121ss: 0.4808 - acc:  - ETA: 18s - lo - E - ETA: 9s - loss: 0.4858 - - ETA: 8s - loss: 0.4864 - ac - ETA: 8s - loss: - ETA: 2s - loss: 0.4859 - acc: 0.9 - ETA: 2s - loss: 0.4858 - acc: 0.953 - ETA: 2s  - ETA: 0s - loss: 0.4864 - acc: 0.953 - ETA: 0s - loss: 0.4862 \n",
      "Epoch 87/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4824 - acc: 0.9526 - val_loss: 0.9707 - val_acc: 0.71024827 - \n",
      "Epoch 88/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4786 - acc: 0.9548 - val_loss: 0.9691 - val_acc: 0.7090\n",
      "Epoch 89/100\n",
      "1562/1562 [==============================] - 28s 18ms/step - loss: 0.4811 - acc: 0.9546 - val_loss: 0.9663 - val_acc: 0.7129\n",
      "Epoch 90/100\n",
      "1562/1562 [==============================] - 27s 18ms/step - loss: 0.4815 - acc: 0.9543 - val_loss: 0.9659 - val_acc: 0.7122ss: 0.4816 - acc: 0.9\n",
      "Epoch 91/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4868 - acc: 0.9525 - val_loss: 0.9795 - val_acc: 0.709666 - a - ETA: 10s - lo - ETA: 0s - loss: 0.4866 - ac\n",
      "Epoch 92/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4808 - acc: 0.9539 - val_loss: 0.9833 - val_acc: 0.7075\n",
      "Epoch 93/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4799 - acc: 0.9543 - val_loss: 0.9658 - val_acc: 0.711279 - ETA: 1s - loss: 0.4797 - acc: 0.9 - ETA: 0s - loss: 0.4797 - acc: 0.954 - ETA: 0s - loss: 0.4797\n",
      "Epoch 94/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4827 - acc: 0.9536 - val_loss: 0.9587 - val_acc: 0.7104 ETA: 9s - l - ETA: 3s - loss: 0.48 - ETA: 2s - loss: - ETA: 1s - loss: \n",
      "Epoch 95/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4760 - acc: 0.9554 - val_loss: 0.9680 - val_acc: 0.7069\n",
      "Epoch 96/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4794 - acc: 0.9559 - val_loss: 0.9579 - val_acc: 0.7110s: 0.4792 - acc: 0.9\n",
      "Epoch 97/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4747 - acc: 0.9552 - val_loss: 0.9654 - val_acc: 0.7075: 0.4769 - a - ETA: 4s - loss: 0.4763 - acc: - ETA: 3s - loss:  - ETA: 2s - loss: 0.4754 - - \n",
      "Epoch 98/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4803 - acc: 0.9540 - val_loss: 0.9741 - val_acc: 0.7084\n",
      "Epoch 99/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4796 - acc: 0.9556 - val_loss: 0.9608 - val_acc: 0.7099\n",
      "Epoch 100/100\n",
      "1562/1562 [==============================] - 27s 17ms/step - loss: 0.4785 - acc: 0.9546 - val_loss: 0.9728 - val_acc: 0.7125 0s - loss: 0.4786 -\n"
     ]
    }
   ],
   "source": [
    "from keras.losses import categorical_crossentropy \n",
    "for name,method in iterator_dict.items():\n",
    "    csvlogger = CSVLogger(\"data/\"+name+\".csv\")\n",
    "    tensorboard = TensorBoard(log_dir=\"overfit/\"+name+\"/\")\n",
    "    model = load_normal()\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adamax(), metrics=['accuracy'])\n",
    "    train_history = model.fit_generator(generator=method,\n",
    "                        validation_data = (x_img_test_normalize,y_label_test_OneHot),\n",
    "                        epochs=100, \n",
    "                        steps_per_epoch=len(x_img_train_normalize)//32,\n",
    "                        callbacks=[csvlogger,tensorboard]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method2 从模型角度出发解决过拟合问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造模型\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Dropout,Flatten,BatchNormalization\n",
    "from keras import regularizers\n",
    "\n",
    "def load_normal():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),\n",
    "                     input_shape=(32, 32,3),\n",
    "                     activation='relu', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# def load_batchNormalization():\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(filters=32,kernel_size=(3,3),\n",
    "#                      input_shape=(32, 32,3),\n",
    "#                      activation='relu', \n",
    "#                      padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "#                      activation='relu', padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(128, activation='relu'))\n",
    "#     model.add(Dense(10, activation='softmax'))\n",
    "#     return model\n",
    "\n",
    "def load_dropout01():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),\n",
    "                     input_shape=(32, 32,3),\n",
    "                     activation='relu', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     activation='relu', padding='same'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def load_dropout025():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),\n",
    "                     input_shape=(32, 32,3),\n",
    "                     activation='relu', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     activation='relu', padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def load_l2normalization025():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),\n",
    "                     input_shape=(32, 32,3),\n",
    "                     kernel_regularizer=regularizers.l2(0.25),\n",
    "                     activation='relu', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     kernel_regularizer=regularizers.l2(0.25),\n",
    "                     activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def load_l2normalization01():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),\n",
    "                     input_shape=(32, 32,3),\n",
    "                     kernel_regularizer=regularizers.l2(0.1),\n",
    "                     activation='relu', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     kernel_regularizer=regularizers.l2(0.1),\n",
    "                     activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def load_l1normalization01():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),\n",
    "                     input_shape=(32, 32,3),\n",
    "                     kernel_regularizer=regularizers.l1(0.1),\n",
    "                     activation='relu', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     kernel_regularizer=regularizers.l1(0.1),\n",
    "                     activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def load_l1normalization025():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32,kernel_size=(3,3),\n",
    "                     input_shape=(32, 32,3),\n",
    "                     kernel_regularizer=regularizers.l1(0.25),\n",
    "                     activation='relu', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     kernel_regularizer=regularizers.l1(0.25),\n",
    "                     activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# models_dict = {\"normal\":load_normal,\n",
    "models_dict = {\n",
    "               \"l1normalization-0.1\":load_l1normalization01,\n",
    "                \"l1normalization-0.25\":load_l1normalization025,\n",
    "                \"l2normalization-0.1\":load_l2normalization01,\n",
    "                \"l2normalization-0.25\":load_l2normalization025,\n",
    "                \"dropout-0.1\":load_dropout01,\n",
    "                \"dropout-0.25\":load_dropout025}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy \n",
    "for name,model in models_dict.items():\n",
    "    csvlogger = CSVLogger(\"new/\"+name+\".csv\")\n",
    "    tensorboard = TensorBoard(log_dir=\"new/\"+name+\"/\")\n",
    "    model = models_dict[name]()\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=adamax, metrics=['accuracy'])\n",
    "    train_history = model.fit(x_img_train_normalize, y_label_train_OneHot,\n",
    "                        validation_data = (x_img_test_normalize,y_label_test_OneHot),\n",
    "                        epochs=100, \n",
    "                        batch_size=64,\n",
    "                        callbacks=[csvlogger,tensorboard]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
